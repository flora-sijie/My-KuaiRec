import pandas as pd
import numpy as np
import os
import pickle
from sklearn.preprocessing import LabelEncoder
from ast import literal_eval

# ================= é…ç½®åŒº =================
DATA_DIR = './data' # ç¡®ä¿è·¯å¾„æ­£ç¡®
OUTPUT_DIR = './processed_data'
SOURCE_FILE = 'small_matrix.csv' # ä¹‹åæ”¹ big_matrix.csv
WATCH_RATIO_THRESHOLD = 0.5 

# åˆ†å±‚æ¯”ä¾‹
HEAD_RATIO = 0.2
MID_RATIO = 0.3

RANDOM_SEED = 2023
np.random.seed(RANDOM_SEED)

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_data_final():
    print(f"ğŸš€ å¼€å§‹æœ€ç»ˆç‰ˆæ•°æ®å¤„ç† (Target: æ•è·é‚£50æ¡æœ‰æ•ˆè¾¹)...")

    # 1. è¯»å–äº¤äº’
    print(f"--- [1/6] è¯»å–äº¤äº’çŸ©é˜µ ({SOURCE_FILE}) ---")
    df_inter = pd.read_csv(os.path.join(DATA_DIR, SOURCE_FILE))
    df_inter = df_inter[df_inter['watch_ratio'] >= WATCH_RATIO_THRESHOLD].copy()
    
    # 2. è¯»å–ä½œè€…ä¿¡æ¯ (å…³è” video_id -> author_id)
    print("--- [2/6] æ„å»º Item-Author æ˜ å°„ ---")
    feat_path = os.path.join(DATA_DIR, 'item_daily_features.csv')
    # åªè¯»ä¸¤åˆ—
    df_item_feat = pd.read_csv(feat_path, usecols=['video_id', 'author_id'])
    # å»é‡
    item2author_raw = df_item_feat.drop_duplicates('video_id')[['video_id', 'author_id']]
    
    # è¿‡æ»¤ï¼šäº¤äº’çŸ©é˜µé‡Œçš„è§†é¢‘å¿…é¡»æœ‰ä½œè€…ä¿¡æ¯
    valid_video_ids = set(item2author_raw.video_id)
    df_inter = df_inter[df_inter['video_id'].isin(valid_video_ids)].copy()
    
    # 3. ID Encoding
    print("--- [3/6] ID ç¼–ç  ---")
    
    # User
    user_encoder = LabelEncoder()
    df_inter['user_idx'] = user_encoder.fit_transform(df_inter['user_id'])
    user_list = user_encoder.classes_
    num_users = len(user_list)
    
    # Item
    item_encoder = LabelEncoder()
    df_inter['item_idx'] = item_encoder.fit_transform(df_inter['video_id'])
    item_list = item_encoder.classes_
    num_items = len(item_list)
    
    # Author
    # é€»è¾‘ï¼šå…ˆå»ºç«‹ map: video_id -> author_id
    raw_vid2aid = dict(zip(item2author_raw.video_id, item2author_raw.author_id))
    # æ‰¾åˆ°æ‰€æœ‰ items å¯¹åº”çš„ raw author id
    relevant_authors_raw = [raw_vid2aid[vid] for vid in item_list]
    
    author_encoder = LabelEncoder()
    author_mapped_ids = author_encoder.fit_transform(relevant_authors_raw) # itemå¯¹åº”çš„author_idxæ•°ç»„
    num_authors = len(author_encoder.classes_)
    
    # å…³é”®æ•°ç»„: item_idx -> author_idx
    item2author_array = author_mapped_ids
    
    # å…³é”®é›†åˆ: ç”¨äºç¤¾äº¤è¿‡æ»¤çš„â€œæœ‰æ•ˆä½œè€…åŸå§‹IDâ€é›†åˆ
    # åªæœ‰åœ¨è¿™ä¸ªé›†åˆé‡Œçš„äººï¼Œæ‰ç®—â€œç”Ÿäº§è€…â€
    valid_author_raw_set = set(author_encoder.classes_)
    
    print(f"   ç»Ÿè®¡: Users={num_users}, Items={num_items}, Authors={num_authors}")

    # 4. User Features (æ´»è·ƒåº¦)
    print("--- [4/6] ç”¨æˆ·æ´»è·ƒåº¦ç‰¹å¾ ---")
    df_user_feat = pd.read_csv(os.path.join(DATA_DIR, 'user_features.csv'))
    # å¡«å……ç©ºå€¼
    df_user_feat['user_active_degree'] = df_user_feat['user_active_degree'].fillna('unknown')
    # å»ºç«‹æ˜ å°„
    u_feat_map = df_user_feat.set_index('user_id')['user_active_degree'].to_dict()
    
    active_encoder = LabelEncoder()
    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾å¹¶ fit
    all_labels = list(df_user_feat['user_active_degree'].unique())
    if 'unknown' not in all_labels: all_labels.append('unknown')
    active_encoder.fit(all_labels)
    num_active_levels = len(active_encoder.classes_)
    unknown_code = active_encoder.transform(['unknown'])[0]
    
    user_active_feature = np.full(num_users, unknown_code, dtype=int)
    for i, u_raw in enumerate(user_list):
        if u_raw in u_feat_map:
            user_active_feature[i] = active_encoder.transform([u_feat_map[u_raw]])[0]

    # 5. ç¤¾äº¤å…³ç³» (Social Edges) - æ ¸å¿ƒä¿®æ­£éƒ¨åˆ†
    print("--- [5/6] æå–ç¤¾äº¤å…³ç³» (User -> Author) ---")
    df_social = pd.read_csv(os.path.join(DATA_DIR, 'social_network.csv'))
    
    social_edges = []
    # è¿™é‡Œçš„ user_list æ˜¯ LabelEncoder é‡Œçš„ classes_ï¼Œå³ raw user ids
    valid_user_raw_set = set(user_list)
    
    for _, row in df_social.iterrows():
        u_raw = row['user_id']
        # 1. å…³æ³¨è€…å¿…é¡»åœ¨æˆ‘ä»¬çš„ç”¨æˆ·é›†é‡Œ
        if u_raw not in valid_user_raw_set:
            continue
            
        try:
            friend_list = literal_eval(row['friend_list'])
        except:
            continue
            
        # è·å–ç¼–ç åçš„ user_idx
        u_idx = user_encoder.transform([u_raw])[0]
        
        for f_raw in friend_list:
            # 2. è¢«å…³æ³¨è€…å¿…é¡»åœ¨æˆ‘ä»¬çš„ä½œè€…é›†é‡Œ (è¿™æ ·æ‰ç®—å…³æ³¨äº†ç”Ÿäº§è€…)
            if f_raw in valid_author_raw_set:
                # è·å–ç¼–ç åçš„ author_idx
                a_idx = author_encoder.transform([f_raw])[0]
                social_edges.append([u_idx, a_idx])
                
    social_edges = np.array(social_edges)
    print(f"âœ… æˆåŠŸæå–ç¤¾äº¤è¾¹: {len(social_edges)} æ¡ (é¢„æœŸåº”æ¥è¿‘ 50)")

    # 6. åˆ›ä½œè€…åˆ†å±‚
    print("--- [6/6] åˆ›ä½œè€…åˆ†å±‚ ---")
    author_heat = np.zeros(num_authors, dtype=int)
    # ç»Ÿè®¡äº¤äº’
    # éå†æ‰€æœ‰äº¤äº’ï¼Œæ‰¾åˆ°å¯¹åº”çš„ item_idx -> æ‰¾åˆ° author_idx -> åŠ çƒ­åº¦
    # æ›´å¿«çš„æ–¹æ³•ï¼š
    item_counts = df_inter['item_idx'].value_counts()
    for i_idx, cnt in item_counts.items():
        a_idx = item2author_array[i_idx]
        author_heat[a_idx] += cnt
        
    sorted_idx = np.argsort(author_heat)[::-1]
    n_head = int(num_authors * HEAD_RATIO)
    n_mid = int(num_authors * MID_RATIO)
    
    author_groups = np.zeros(num_authors, dtype=int)
    author_groups[sorted_idx[n_head:n_head+n_mid]] = 1 # Mid
    author_groups[sorted_idx[:n_head]] = 2 # Head
    
    # 7. ä¿å­˜
    indices = np.arange(len(df_inter))
    np.random.shuffle(indices)
    split = int(len(indices) * 0.8)
    
    dataset = {
        'num_users': int(num_users),
        'num_items': int(num_items),
        'num_authors': int(num_authors),
        'num_active_levels': int(num_active_levels),
        'item2author': item2author_array,
        'user_active_feature': user_active_feature,
        'author_groups': author_groups,
        'social_edges': social_edges, # è¿™é‡Œå­˜çš„ä¸€å®šæ˜¯ [user_idx, author_idx]
        'train_pairs': df_inter.iloc[indices[:split]][['user_idx', 'item_idx']].values,
        'test_pairs': df_inter.iloc[indices[split:]][['user_idx', 'item_idx']].values
    }
    
    with open(os.path.join(OUTPUT_DIR, 'dataset.pkl'), 'wb') as f:
        pickle.dump(dataset, f)
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼dataset.pkl å·²ç”Ÿæˆã€‚")

if __name__ == '__main__':
    process_data_final()