import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
import scipy.sparse as sp
import os
import time

# ================= é…ç½®ä¸è¶…å‚æ•° =================
class Config:
    # è·¯å¾„é…ç½®
    dataset_path = './processed_data/dataset.pkl'
    
    # è®­ç»ƒå‚æ•°
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    epochs = 50
    batch_size = 2048
    learning_rate = 0.001
    weight_decay = 1e-4
    
    # æ¨¡å‹å‚æ•° (XSimGCL)
    emb_dim = 64
    n_layers = 3
    cl_rate = 0.2    # å¯¹æ¯”å­¦ä¹ æƒé‡ lambda
    eps = 0.1        # å™ªå£°æ¯”ä¾‹ epsilon
    temp = 0.2       # æ¸©åº¦ç³»æ•° tau
    
    # è¯„ä¼°å‚æ•°
    top_k = 20

config = Config()
print(f"ğŸš€ è¿è¡Œç¯å¢ƒ: {config.device}")

# ================= 1. æ•°æ®é›†åŠ è½½ä¸å›¾æ„å»º =================
class RecDataset(Dataset):
    def __init__(self, conf):
        print(f"Loading data from {conf.dataset_path}...")
        with open(conf.dataset_path, 'rb') as f:
            data = pickle.load(f)
            
        self.num_users = data['num_users']
        self.num_items = data['num_items']
        self.num_authors = data['num_authors']
        
        # æ ¸å¿ƒæ˜ å°„ï¼šItem -> Author (numpy -> tensor)
        self.item2author = torch.LongTensor(data['item2author']).to(conf.device)
        
        # è®­ç»ƒæ•°æ®
        self.train_pairs = np.array(data['train_pairs'])
        self.test_pairs = data['test_pairs']
        
        # è½¬æ¢æˆ User-Item å­—å…¸ä¾›å¿«é€ŸæŸ¥æ‰¾ï¼ˆç”¨äºè´Ÿé‡‡æ ·å’Œæµ‹è¯•ï¼‰
        self.train_dict = {}
        for u, i in self.train_pairs:
            if u not in self.train_dict: self.train_dict[u] = []
            self.train_dict[u].append(i)
            
        self.test_dict = {}
        for u, i in self.test_pairs:
            if u not in self.test_dict: self.test_dict[u] = []
            self.test_dict[u].append(i)
            
        # æ„å»ºç¨€ç–é‚»æ¥çŸ©é˜µ (ç”¨äº GNN)
        self.graph = self._build_sparse_graph()

    def _build_sparse_graph(self):
        print("æ„å»ºç¨€ç–é‚»æ¥çŸ©é˜µ (Normalized Adjacency Matrix)...")
        # 1. æ„å»ºäº¤äº’çŸ©é˜µ R (User x Item)
        u_ids = self.train_pairs[:, 0]
        i_ids = self.train_pairs[:, 1]
        values = np.ones(len(self.train_pairs), dtype=np.float32)
        
        R = sp.coo_matrix((values, (u_ids, i_ids)), shape=(self.num_users, self.num_items))
        
        # 2. æ„å»ºå¤§é‚»æ¥çŸ©é˜µ A
        # [0, R]
        # [R.T, 0]
        vals = np.concatenate([values, values])
        rows = np.concatenate([u_ids, i_ids + self.num_users])
        cols = np.concatenate([i_ids + self.num_users, u_ids])
        
        adj_shape = self.num_users + self.num_items
        adj = sp.coo_matrix((vals, (rows, cols)), shape=(adj_shape, adj_shape))
        
        # 3. å½’ä¸€åŒ– D^-0.5 * A * D^-0.5
        rowsum = np.array(adj.sum(1))
        d_inv = np.power(rowsum, -0.5).flatten()
        d_inv[np.isinf(d_inv)] = 0.
        d_mat_inv = sp.diags(d_inv)
        
        norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)
        
        # 4. è½¬ä¸º PyTorch Sparse Tensor
        norm_adj = norm_adj.tocoo()
        indices = np.vstack((norm_adj.row, norm_adj.col))
        i = torch.LongTensor(indices)
        v = torch.FloatTensor(norm_adj.data)
        shape = norm_adj.shape
        
        graph = torch.sparse.FloatTensor(i, v, torch.Size(shape)).to(config.device)
        return graph

    def __len__(self):
        return len(self.train_pairs)
    
    def __getitem__(self, idx):
        # ç®€å•çš„æ­£æ ·æœ¬æå–ï¼Œè´Ÿé‡‡æ ·åœ¨ collate_fn æˆ– è®­ç»ƒå¾ªç¯é‡Œåšæ›´é«˜æ•ˆ
        # è¿™é‡Œä¸ºäº†é…åˆ DataLoaderï¼Œæˆ‘ä»¬è¿”å› user, pos_item
        u, i = self.train_pairs[idx]
        return u, i

# ================= 2. æ¨¡å‹å®šä¹‰ (CreatorXSimGCL) =================
class CreatorXSimGCL(nn.Module):
    def __init__(self, dataset, conf):
        super(CreatorXSimGCL, self).__init__()
        self.num_users = dataset.num_users
        self.num_items = dataset.num_items
        self.num_authors = dataset.num_authors
        self.graph = dataset.graph
        
        self.emb_dim = conf.emb_dim
        self.n_layers = conf.n_layers
        self.eps = conf.eps
        self.item2author_map = dataset.item2author
        
        # Embeddings
        self.user_emb = nn.Embedding(self.num_users, self.emb_dim)
        self.item_emb = nn.Embedding(self.num_items, self.emb_dim)
        self.author_emb = nn.Embedding(self.num_authors, self.emb_dim)
        
        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.user_emb.weight)
        nn.init.xavier_uniform_(self.item_emb.weight)
        nn.init.xavier_uniform_(self.author_emb.weight)

    def forward(self, perturbed=False):
        # 1. èåˆåˆ›ä½œè€…ä¿¡æ¯ (Part A æ ¸å¿ƒåˆ›æ–°)
        # Aï¼šCreator-Aware
        # item_emb = item_id_emb + author_id_emb
        #author_feats = self.author_emb(self.item2author_map) # [num_items, dim]
        #mixed_item_emb = self.item_emb.weight + author_feats
        
        # Bï¼šbaseline-XSimGCL
        mixed_item_emb = self.item_emb.weight  # ç›´æ¥ç”¨ ID Embedding
        
        # 2. æ‹¼æ¥åˆå§‹ç‰¹å¾
        ego_embeddings = torch.cat([self.user_emb.weight, mixed_item_emb], dim=0)
        all_embeddings = []
        
        # 3. å›¾å·ç§¯ä¼ æ’­
        for k in range(self.n_layers):
            ego_embeddings = torch.sparse.mm(self.graph, ego_embeddings)
            
            # XSimGCL æ ¸å¿ƒï¼šåŠ å…¥éšæœºå™ªå£°
            if perturbed:
                noise = torch.rand_like(ego_embeddings).to(ego_embeddings.device)
                # sign(E) * noise * eps
                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(noise, dim=-1) * self.eps
                
            all_embeddings.append(ego_embeddings)
            
        # 4. èšåˆå±‚ (Mean Pooling)
        final_embeddings = torch.stack(all_embeddings, dim=1)
        final_embeddings = torch.mean(final_embeddings, dim=1)
        
        # 5. æ‹†åˆ†å› User å’Œ Item
        users, items = torch.split(final_embeddings, [self.num_users, self.num_items])
        return users, items

# ================= 3. å·¥å…·å‡½æ•° (Loss & Evaluation) =================
def cal_bpr_loss(user_emb, pos_item_emb, neg_item_emb):
    pos_score = torch.mul(user_emb, pos_item_emb).sum(dim=1)
    neg_score = torch.mul(user_emb, neg_item_emb).sum(dim=1)
    loss = -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8)
    return torch.mean(loss)

def cal_infonce_loss(view1, view2, temperature):
    # InfoNCE Loss: L = -log( exp(sim(v1, v2)/t) / sum(exp(sim(v1, all)/t)) )
    # ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œé€šå¸¸ä½¿ç”¨ Batch å†…è´Ÿé‡‡æ ·
    view1 = F.normalize(view1, dim=1)
    view2 = F.normalize(view2, dim=1)
    
    pos_score = (view1 * view2).sum(dim=1) / temperature
    pos_score = torch.exp(pos_score)
    
    # çŸ©é˜µä¹˜æ³•è®¡ç®— Batch å†…æ‰€æœ‰ç›¸ä¼¼åº¦
    ttl_score = torch.matmul(view1, view2.transpose(0, 1)) / temperature
    ttl_score = torch.exp(ttl_score).sum(dim=1)
    
    loss = -torch.log(pos_score / ttl_score + 1e-8)
    return torch.mean(loss)

def evaluate(model, dataset, top_k=20):
    model.eval()
    NDCG, RECALL = [], []
    test_users = list(dataset.test_dict.keys())
    
    with torch.no_grad():
        # è·å–æœ€ç»ˆçš„ User å’Œ Item Embedding (ä¸åŠ å™ªå£°)
        all_users, all_items = model(perturbed=False)
        
        # åˆ†æ‰¹æ¬¡æµ‹è¯•é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
        batch_size = 100
        for start in range(0, len(test_users), batch_size):
            end = min(start + batch_size, len(test_users))
            batch_u_ids = test_users[start:end]
            
            # è·å–å½“å‰ Batch User çš„å‘é‡
            batch_u_emb = all_users[batch_u_ids]
            
            # è®¡ç®—æ‰€æœ‰ Item çš„åˆ†æ•°
            scores = torch.matmul(batch_u_emb, all_items.transpose(0, 1))
            
            # Mask æ‰è®­ç»ƒé›†ä¸­å·²ç»çœ‹è¿‡çš„ç‰©å“ (é˜²æ­¢ä½œå¼Š)
            for i, u_id in enumerate(batch_u_ids):
                train_pos = dataset.train_dict.get(u_id, [])
                scores[i, train_pos] = -1e9 # è®¾ç½®ä¸ºæå°å€¼
            
            # Top-K æ’åº
            _, indices = torch.topk(scores, top_k, dim=1)
            indices = indices.cpu().numpy()
            
            # è®¡ç®—æŒ‡æ ‡
            for i, u_id in enumerate(batch_u_ids):
                ground_truth = set(dataset.test_dict[u_id])
                hit = 0
                idcg = 0
                dcg = 0
                
                for j, item_idx in enumerate(indices[i]):
                    if item_idx in ground_truth:
                        hit += 1
                        dcg += 1.0 / np.log2(j + 2)
                    idcg += 1.0 / np.log2(j + 2)
                
                # åªæœ‰å½“ ground_truth é‡Œçš„æ•°é‡å°‘äº k æ—¶ï¼ŒIDCG æ‰æ˜¯éƒ¨åˆ†å’Œ
                # å‡†ç¡®çš„ IDCG åº”è¯¥æ˜¯å‰ min(len(gt), k) ä¸ªä½ç½®ä¸º 1
                real_idcg = 0
                for j in range(min(len(ground_truth), top_k)):
                    real_idcg += 1.0 / np.log2(j + 2)
                    
                RECALL.append(hit / len(ground_truth))
                NDCG.append(dcg / real_idcg if real_idcg > 0 else 0)
                
    return np.mean(RECALL), np.mean(NDCG)

# ================= 4. ä¸»è®­ç»ƒå¾ªç¯ (Main) =================
if __name__ == '__main__':
    # 1. åˆå§‹åŒ–
    dataset = RecDataset(config)
    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    
    model = CreatorXSimGCL(dataset, config).to(config.device)
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    
    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ (Epochs={config.epochs})...")
    
    best_recall = 0
    
    for epoch in range(config.epochs):
        model.train()
        total_loss = 0
        
        start_time = time.time()
        
        for batch_u, batch_pos_i in dataloader:
            batch_u = batch_u.to(config.device)
            batch_pos_i = batch_pos_i.to(config.device)
            
            # 2. è´Ÿé‡‡æ · (ç®€å•çš„éšæœºé‡‡æ ·)
            batch_neg_i = torch.randint(0, dataset.num_items, batch_pos_i.shape).to(config.device)
            # (ä¸¥è°¨çš„åšæ³•åº”è¯¥æ£€æŸ¥ neg æ˜¯å¦åœ¨ train_dict ä¸­ï¼Œè¿™é‡Œä¸ºäº†é€Ÿåº¦ç•¥è¿‡)
            
            # 3. è®¡ç®—æ¨è Loss (Clean View)
            users_emb, items_emb = model(perturbed=False)
            
            u_e = users_emb[batch_u]
            pos_i_e = items_emb[batch_pos_i]
            neg_i_e = items_emb[batch_neg_i]
            
            rec_loss = cal_bpr_loss(u_e, pos_i_e, neg_i_e)
            
            # 4. è®¡ç®—å¯¹æ¯”å­¦ä¹  Loss (XSimGCL)
            # ç”Ÿæˆä¸¤ä¸ªæœ‰å™ªå£°çš„è§†å›¾
            users_view1, items_view1 = model(perturbed=True)
            users_view2, items_view2 = model(perturbed=True)
            
            # åªè®¡ç®—å½“å‰ Batch æ¶‰åŠèŠ‚ç‚¹çš„ CL Lossï¼Œå‡å°‘è®¡ç®—é‡
            # User CL
            cl_loss_u = cal_infonce_loss(users_view1[batch_u], users_view2[batch_u], config.temp)
            # Item CL (Pos Items)
            cl_loss_i = cal_infonce_loss(items_view1[batch_pos_i], items_view2[batch_pos_i], config.temp)
            
            cl_loss = config.cl_rate * (cl_loss_u + cl_loss_i)
            
            # 5. åå‘ä¼ æ’­
            loss = rec_loss + cl_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        # æ¯ä¸ª Epoch ç»“æŸåè¯„ä¼°
        recall, ndcg = evaluate(model, dataset, config.top_k)
        print(f"Epoch {epoch+1:2d} | Loss: {total_loss/len(dataloader):.4f} | "
              f"Recall@{config.top_k}: {recall:.4f} | NDCG@{config.top_k}: {ndcg:.4f} | "
              f"Time: {time.time()-start_time:.1f}s")
        
        if recall > best_recall:
            best_recall = recall
            # ä¿å­˜æ¨¡å‹ (å¯é€‰)
            # torch.save(model.state_dict(), 'best_model.pth')

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Best Recall: {best_recall:.4f}")