import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
import scipy.sparse as sp
import os
import time

# ================= é…ç½®åŒº =================
class Config:
    dataset_path = './processed_data/dataset.pkl'
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    epochs = 50
    batch_size = 2048
    lr_worker = 0.001
    lr_manager = 0.005 # Manager é€šå¸¸éœ€è¦æ›´å¤§çš„æ›´æ–°å¹…åº¦
    weight_decay = 1e-4
    
    # Worker å‚æ•°
    emb_dim = 64
    n_layers = 3
    cl_rate = 0.2
    eps = 0.1
    temp = 0.2
    
    # Manager å‚æ•° (ç”Ÿæ€è°ƒèŠ‚)
    eco_lambda = 0.5   # ç”Ÿæ€ Loss çš„æƒé‡ (è®ºæ–‡ä¸­çš„ lambda)
    target_exposure = 0.3 # ç›®æ ‡é•¿å°¾æ›å…‰ç‡ (è®ºæ–‡ä¸­çš„ tau)
    
    # è¯„ä¼°
    top_k = 20

config = Config()
print(f"ğŸš€ Phase 2: Full Model Training on {config.device}")

# ================= 1. æ•°æ®é›† (Dataset) =================
class RecDataset(Dataset):
    def __init__(self, conf):
        with open(conf.dataset_path, 'rb') as f:
            data = pickle.load(f)
            
        self.num_users = data['num_users']
        self.num_items = data['num_items']
        self.num_authors = data['num_authors']
        self.num_active_levels = data['num_active_levels']
        
        # æ ¸å¿ƒæ˜ å°„
        self.item2author = torch.LongTensor(data['item2author']).to(conf.device)
        self.author_groups = torch.LongTensor(data['author_groups']).to(conf.device)
        self.user_active = torch.LongTensor(data['user_active_feature']).to(conf.device)
        
        # ç”Ÿæˆ Item-level çš„ Tail Mask (æ–¹ä¾¿æŸ¥è¯¢)
        # author_groups: 0=Tail, 1=Mid, 2=Head
        # æˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ª item æ˜¯ä¸æ˜¯ Tail
        # é€»è¾‘: item -> author -> group
        item_groups = self.author_groups[self.item2author]
        self.is_tail_item = (item_groups == 0).float() # [num_items] 1.0 if tail
        
        self.train_pairs = np.array(data['train_pairs'])
        self.test_pairs = data['test_pairs']
        
        self.train_dict = {}
        for u, i in self.train_pairs:
            if u not in self.train_dict: self.train_dict[u] = []
            self.train_dict[u].append(i)
        self.test_dict = {}
        for u, i in self.test_pairs:
            if u not in self.test_dict: self.test_dict[u] = []
            self.test_dict[u].append(i)
            
        self.graph = self._build_sparse_graph()

    def _build_sparse_graph(self):
        u_ids = self.train_pairs[:, 0]
        i_ids = self.train_pairs[:, 1]
        values = np.ones(len(self.train_pairs), dtype=np.float32)
        R = sp.coo_matrix((values, (u_ids, i_ids)), shape=(self.num_users, self.num_items))
        
        vals = np.concatenate([values, values])
        rows = np.concatenate([u_ids, i_ids + self.num_users])
        cols = np.concatenate([i_ids + self.num_users, u_ids])
        adj_shape = self.num_users + self.num_items
        adj = sp.coo_matrix((vals, (rows, cols)), shape=(adj_shape, adj_shape))
        
        rowsum = np.array(adj.sum(1))
        d_inv = np.power(rowsum + 1e-9, -0.5).flatten() # Fix divide by zero
        d_mat_inv = sp.diags(d_inv)
        norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)
        
        norm_adj = norm_adj.tocoo()
        i = torch.LongTensor(np.vstack((norm_adj.row, norm_adj.col)))
        v = torch.FloatTensor(norm_adj.data)
        return torch.sparse_coo_tensor(i, v, torch.Size(norm_adj.shape)).to(config.device)

    def __len__(self): return len(self.train_pairs)
    def __getitem__(self, idx): return self.train_pairs[idx][0], self.train_pairs[idx][1]

# ================= 2. æ¨¡å‹å®šä¹‰ (Worker + Manager) =================

# --- Part B: The Manager ---
class ManagerNetwork(nn.Module):
    def __init__(self, num_active_levels, emb_dim=32):
        super(ManagerNetwork, self).__init__()
        # Input: User Active Level (Embedding)
        self.active_emb = nn.Embedding(num_active_levels, emb_dim)
        
        # MLP: State -> Weight
        self.net = nn.Sequential(
            nn.Linear(emb_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 1),
            nn.Sigmoid() # è¾“å‡º 0~1 ä¹‹é—´çš„æ‰¶æŒåŠ›åº¦
        )
        
    def forward(self, active_level_ids):
        # active_level_ids: [batch_size]
        emb = self.active_emb(active_level_ids)
        weight = self.net(emb) # [batch_size, 1]
        return weight

# --- Part A: The Worker (CreatorXSimGCL) ---
class CreatorXSimGCL(nn.Module):
    def __init__(self, dataset, conf):
        super(CreatorXSimGCL, self).__init__()
        self.num_users = dataset.num_users
        self.num_items = dataset.num_items
        self.num_authors = dataset.num_authors
        self.graph = dataset.graph
        self.eps = conf.eps
        self.n_layers = conf.n_layers
        
        self.item2author_map = dataset.item2author
        
        self.user_emb = nn.Embedding(self.num_users, conf.emb_dim)
        self.item_emb = nn.Embedding(self.num_items, conf.emb_dim)
        self.author_emb = nn.Embedding(self.num_authors, conf.emb_dim)
        
        nn.init.xavier_uniform_(self.user_emb.weight)
        nn.init.xavier_uniform_(self.item_emb.weight)
        nn.init.xavier_uniform_(self.author_emb.weight)

    def forward(self, perturbed=False):
        # Creator-Aware Fusion
        author_feats = self.author_emb(self.item2author_map)
        mixed_item_emb = self.item_emb.weight + author_feats
        
        ego_embeddings = torch.cat([self.user_emb.weight, mixed_item_emb], dim=0)
        all_embeddings = []
        
        for k in range(self.n_layers):
            ego_embeddings = torch.sparse.mm(self.graph, ego_embeddings)
            if perturbed:
                noise = torch.rand_like(ego_embeddings)
                ego_embeddings += torch.sign(ego_embeddings) * F.normalize(noise, dim=-1) * self.eps
            all_embeddings.append(ego_embeddings)
            
        final_embeddings = torch.stack(all_embeddings, dim=1).mean(dim=1)
        return torch.split(final_embeddings, [self.num_users, self.num_items])

# ================= 3. è”åˆè®­ç»ƒæ¡†æ¶ =================
class JointModel(nn.Module):
    def __init__(self, dataset, conf):
        super(JointModel, self).__init__()
        self.worker = CreatorXSimGCL(dataset, conf)
        self.manager = ManagerNetwork(dataset.num_active_levels)
        self.dataset = dataset
        self.conf = conf
        
    def predict(self, u_batch, i_batch, use_manager=True):
        # 1. Worker Score (Base)
        users_emb, items_emb = self.worker() # No perturbation during inference
        
        u_e = users_emb[u_batch]
        i_e = items_emb[i_batch]
        base_score = (u_e * i_e).sum(dim=1)
        
        if not use_manager:
            return base_score
        
        # 2. Manager Boost
        # è·å–ç”¨æˆ·æ´»è·ƒåº¦
        u_active = self.dataset.user_active[u_batch]
        boost_weight = self.manager(u_active).squeeze() # [batch]
        
        # è·å–ç‰©å“æ˜¯å¦ä¸º Tail
        is_tail = self.dataset.is_tail_item[i_batch] # [batch] 0 or 1
        
        # æœ€ç»ˆå¾—åˆ† = åŸºç¡€åˆ† + æ‰¶æŒåˆ†
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®© Manager å†³å®š"æ˜¯å¦"æ‰¶æŒä»¥åŠ"æ‰¶æŒå¤šå°‘"
        # åªæœ‰å½“ç‰©å“æ˜¯ Tail æ—¶ï¼Œæ‰¶æŒæ‰ç”Ÿæ•ˆ
        final_score = base_score + (boost_weight * is_tail)
        
        return final_score

# ================= 4. Loss Functions =================
def cal_bpr_loss(scores_pos, scores_neg):
    # åŸºç¡€ BPR: log(sigmoid(pos - neg))
    loss = -torch.log(torch.sigmoid(scores_pos - scores_neg) + 1e-8)
    return loss.mean()

def cal_infonce_loss(view1, view2, temp):
    view1 = F.normalize(view1, dim=1)
    view2 = F.normalize(view2, dim=1)
    pos_score = (view1 * view2).sum(dim=1) / temp
    pos_score = torch.exp(pos_score)
    ttl_score = torch.matmul(view1, view2.transpose(0, 1)) / temp
    ttl_score = torch.exp(ttl_score).sum(dim=1)
    return -torch.log(pos_score / ttl_score + 1e-8).mean()

def cal_eco_loss(scores_all_items, is_tail_mask, target_exposure, k=20):
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç”Ÿæ€ Loss å®ç°
    # æˆ‘ä»¬å¸Œæœ› Top-K ä¸­ Tail çš„æ¯”ä¾‹æ¥è¿‘ target_exposure
    # ä½†ç”±äº argmax/topk ä¸å¯å¯¼ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨ Softmax è¿‘ä¼¼
    
    # è¿™é‡Œçš„å®ç°æ¯”è¾ƒ Trickï¼šæˆ‘ä»¬åªæƒ©ç½š"Manageræ²¡æœ‰ç»™Tailè¶³å¤Ÿåˆ†å€¼"çš„æƒ…å†µ
    # ä½†åœ¨ Batch è®­ç»ƒä¸­ï¼Œè®¡ç®—å…¨å±€ TopK å¤ªæ…¢ã€‚
    # æ›¿ä»£æ–¹æ¡ˆï¼šæœ€å¤§åŒ– (Manager_Weight * Tail_Items) çš„å‡å€¼ï¼Œç›´è‡³è¾¾åˆ°é˜ˆå€¼
    # æˆ–è€…ï¼šä½¿ç”¨ Pairwise æ€æƒ³ï¼Œå¦‚æœ pos æ˜¯ tailï¼Œneg æ˜¯ headï¼Œä¸” score_pos < score_negï¼Œåˆ™å¤§åŠ›æƒ©ç½š
    
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨è®ºæ–‡ä¸­æåˆ°çš„ï¼šExpected Exposure Loss
    # prob = softmax(scores / temp)
    # exposure = sum(prob * is_tail)
    # loss = max(0, target - exposure)
    
    # ä¸ºäº†æ˜¾å­˜ï¼Œæˆ‘ä»¬åªåœ¨ä¸€ä¸ªéšæœºé‡‡æ ·çš„å°å­é›†ä¸Šç®— softmax
    probs = F.softmax(scores_all_items, dim=1)
    expected_tail_exposure = (probs * is_tail_mask).sum(dim=1).mean()
    
    loss = F.relu(target_exposure - expected_tail_exposure)
    return loss

# ================= 5. è®­ç»ƒä¸è¯„ä¼° Loop =================
def evaluate(model, dataset, top_k=20):
    model.eval()
    RECALL, NDCG, TAIL_RATIO = [], [], []
    test_users = list(dataset.test_dict.keys())
    
    with torch.no_grad():
        # é¢„è®¡ç®—æ‰€æœ‰ User å’Œ Item Embedding
        all_users, all_items = model.worker()
        
        for start in range(0, len(test_users), 100):
            end = min(start + 100, len(test_users))
            batch_u_ids = torch.LongTensor(test_users[start:end]).to(config.device)
            
            # --- Inference Logic ---
            # 1. Base Scores
            batch_u_emb = all_users[batch_u_ids]
            scores = torch.matmul(batch_u_emb, all_items.transpose(0, 1))
            
            # 2. Manager Boost (å¹¿æ’­æœºåˆ¶)
            u_active = dataset.user_active[batch_u_ids]
            weights = model.manager(u_active) # [batch, 1]
            is_tail = dataset.is_tail_item.unsqueeze(0) # [1, n_items]
            
            # Final Scores
            scores = scores + (weights * is_tail)
            
            # Mask train
            for i, u_id in enumerate(batch_u_ids.cpu().numpy()):
                train_pos = dataset.train_dict.get(u_id, [])
                scores[i, train_pos] = -1e9
            
            # TopK
            _, indices = torch.topk(scores, top_k, dim=1)
            indices = indices.cpu().numpy()
            
            # Metrics
            for i, u_id in enumerate(batch_u_ids.cpu().numpy()):
                ground_truth = set(dataset.test_dict[u_id])
                hit = 0
                dcg = 0
                idcg = 0
                tail_cnt = 0
                
                for j, item_idx in enumerate(indices[i]):
                    if item_idx in ground_truth:
                        hit += 1
                        dcg += 1.0 / np.log2(j + 2)
                    # ç»Ÿè®¡æ¨èåˆ—è¡¨é‡Œçš„é•¿å°¾å«é‡
                    if dataset.is_tail_item[item_idx] == 1:
                        tail_cnt += 1
                        
                    if j < len(ground_truth):
                        idcg += 1.0 / np.log2(j + 2)
                        
                RECALL.append(hit / len(ground_truth))
                NDCG.append(dcg / idcg if idcg > 0 else 0)
                TAIL_RATIO.append(tail_cnt / top_k)
                
    return np.mean(RECALL), np.mean(NDCG), np.mean(TAIL_RATIO)

if __name__ == '__main__':
    dataset = RecDataset(config)
    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    
    model = JointModel(dataset, config).to(config.device)
    optimizer = optim.Adam([
        {'params': model.worker.parameters(), 'lr': config.lr_worker},
        {'params': model.manager.parameters(), 'lr': config.lr_manager}
    ], weight_decay=config.weight_decay)
    
    print("ğŸ”¥ Start Joint Training (Worker + Manager)...")
    
    for epoch in range(config.epochs):
        model.train()
        total_loss = 0
        
        for batch_u, batch_pos_i in dataloader:
            batch_u = batch_u.to(config.device)
            batch_pos_i = batch_pos_i.to(config.device)
            batch_neg_i = torch.randint(0, dataset.num_items, batch_pos_i.shape).to(config.device)
            
            # --- 1. Forward Pass (Accuracy) ---
            # è¿™é‡Œçš„ BPR Loss éœ€è¦åŸºäº"æœ€ç»ˆå¾—åˆ†"è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯ Manager å¹²é¢„åçš„å¾—åˆ†
            score_pos = model.predict(batch_u, batch_pos_i, use_manager=True)
            score_neg = model.predict(batch_u, batch_neg_i, use_manager=True)
            
            acc_loss = cal_bpr_loss(score_pos, score_neg)
            
            # --- 2. Contrastive Loss (Worker Only) ---
            u_v1, i_v1 = model.worker(perturbed=True)
            u_v2, i_v2 = model.worker(perturbed=True)
            cl_loss = config.cl_rate * (
                cal_infonce_loss(u_v1[batch_u], u_v2[batch_u], config.temp) +
                cal_infonce_loss(i_v1[batch_pos_i], i_v2[batch_pos_i], config.temp)
            )
            
            # --- 3. Ecosystem Loss (Manager Only) ---
            # ä¸ºäº†è®© Manager çœŸçš„å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦æƒ©ç½šå®ƒå¦‚æœä¸æ¨é•¿å°¾
            # æˆ‘ä»¬éšæœºé‡‡æ ·ä¸€äº›ç”¨æˆ·ï¼Œçœ‹ä»–ä»¬çš„ TopK æ¨èé‡Œé•¿å°¾å¤Ÿä¸å¤Ÿ
            # (ç”±äºè®¡ç®—é‡å¤§ï¼Œè¿™é‡Œç®€åŒ–ä¸ºï¼šå¦‚æœ Positive Sample æ˜¯ Tailï¼Œåˆ™ç»™é¢å¤–å¥–åŠ±)
            # æˆ–è€…ç›´æ¥ç”¨ Weights çš„ L2 æ­£åˆ™ï¼Œé˜²æ­¢å®ƒå˜å¾—æ— é™å¤§
            
            # ç®€æ˜“ç‰ˆ Eco Loss: å¼ºè¿« Manager è¾“å‡ºçš„ weight å‡å€¼æ¥è¿‘ 0.5 (è¡¨ç¤ºè‡³å°‘è¦æœ‰ä¸€åŠåŠ›åº¦)
            # æˆ–è€…æ˜¯æ ¹æ® batch å†…å®é™…çš„ tail æ›å…‰æ¥ç®—
            
            # è¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªåŸºäº Margin çš„ Lossï¼š
            # å¦‚æœ pos item æ˜¯ Tailï¼Œæˆ‘ä»¬å¸Œæœ› score_pos è¶Šå¤§è¶Šå¥½
            is_tail_pos = dataset.is_tail_item[batch_pos_i]
            # eco_loss = -torch.mean(score_pos * is_tail_pos) * 0.1 # ç®€å•æ¿€åŠ±
            
            # æ›´é«˜çº§ï¼šHinge Loss
            # ç¡®ä¿ Tail Items çš„å¾—åˆ†æœ‰ä¸€ä¸ªåº•çº¿
            eco_loss = 0
            if is_tail_pos.sum() > 0:
                eco_loss = F.relu(1.0 - score_pos[is_tail_pos.bool()]).mean() * config.eco_lambda
            
            loss = acc_loss + cl_loss + eco_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        # Eval
        recall, ndcg, tail_ratio = evaluate(model, dataset)
        print(f"Epoch {epoch+1:2d} | Loss: {total_loss/len(dataloader):.4f} | "
              f"Recall: {recall:.4f} | NDCG: {ndcg:.4f} | "
              f"TailRatio: {tail_ratio:.4f}") # å…³æ³¨è¿™ä¸ªæŒ‡æ ‡ï¼