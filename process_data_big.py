import pandas as pd
import numpy as np
import os
import pickle
from sklearn.preprocessing import LabelEncoder
from ast import literal_eval
import gc # å¼•å…¥åƒåœ¾å›æ”¶ï¼Œé˜²æ­¢å†…å­˜çˆ†æ‰

# ================= é…ç½®åŒº (Big Matrix) =================
DATA_DIR = './data'
OUTPUT_DIR = './processed_data'
SOURCE_FILE = 'big_matrix.csv' # ğŸ‘ˆ è¿™é‡Œæ”¹æˆäº†å…¨é‡æ•°æ®
OUTPUT_FILE = 'dataset_big.pkl' # ğŸ‘ˆ è¿™é‡Œæ”¹åäº†ï¼Œä¸ä¼šè¦†ç›– dataset.pkl

# è¿‡æ»¤é˜ˆå€¼
WATCH_RATIO_THRESHOLD = 0.5 
HEAD_RATIO = 0.2
MID_RATIO = 0.3
RANDOM_SEED = 2023
np.random.seed(RANDOM_SEED)

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_data_big():
    print(f"ğŸš€ å¼€å§‹å…¨é‡æ•°æ®å¤„ç† (Big Matrix)...")
    print(f"âš ï¸ æ³¨æ„ï¼šå…¨é‡æ•°æ®é‡å¤§ï¼Œè¯·å…³æ³¨ Colab å†…å­˜ä½¿ç”¨æƒ…å†µ")

    # 1. è¯»å–äº¤äº’ (åˆ†å—è¯»å–æˆ–åªè¯»å¿…è¦åˆ—ä»¥èŠ‚çœå†…å­˜)
    print(f"--- [1/6] è¯»å–äº¤äº’çŸ©é˜µ ({SOURCE_FILE}) ---")
    file_path = os.path.join(DATA_DIR, SOURCE_FILE)
    
    # ä¼˜åŒ–ï¼šåªè¯»å–éœ€è¦çš„åˆ—
    use_cols = ['user_id', 'video_id', 'watch_ratio']
    df_inter = pd.read_csv(file_path, usecols=use_cols)
    
    # è¿‡æ»¤
    original_len = len(df_inter)
    df_inter = df_inter[df_inter['watch_ratio'] >= WATCH_RATIO_THRESHOLD].copy()
    print(f"    - åŸå§‹: {original_len} -> è¿‡æ»¤å: {len(df_inter)}")
    
    # 2. Item -> Author æ˜ å°„
    print("--- [2/6] æ„å»º Item-Author æ˜ å°„ ---")
    feat_path = os.path.join(DATA_DIR, 'item_daily_features.csv')
    # åªè¯»ä¸¤åˆ—
    df_item_feat = pd.read_csv(feat_path, usecols=['video_id', 'author_id'])
    item2author_raw = df_item_feat.drop_duplicates('video_id')[['video_id', 'author_id']]
    
    # è¿‡æ»¤ï¼šäº¤äº’ä¸­çš„è§†é¢‘å¿…é¡»æœ‰ä½œè€…
    valid_video_ids = set(item2author_raw.video_id)
    df_inter = df_inter[df_inter['video_id'].isin(valid_video_ids)].copy()
    
    # é‡Šæ”¾ä¸å†éœ€è¦çš„å†…å­˜
    del df_item_feat
    gc.collect()

    # 3. ID Encoding
    print("--- [3/6] ID ç¼–ç  ---")
    
    # User
    user_encoder = LabelEncoder()
    df_inter['user_idx'] = user_encoder.fit_transform(df_inter['user_id'])
    num_users = len(user_encoder.classes_)
    
    # Item
    item_encoder = LabelEncoder()
    df_inter['item_idx'] = item_encoder.fit_transform(df_inter['video_id'])
    num_items = len(item_encoder.classes_)
    
    # Author
    raw_vid2aid = dict(zip(item2author_raw.video_id, item2author_raw.author_id))
    # åªè½¬æ¢åœ¨ df_inter é‡Œå‡ºç°è¿‡çš„ item å¯¹åº”çš„ author
    relevant_authors_raw = [raw_vid2aid[vid] for vid in item_encoder.classes_]
    
    author_encoder = LabelEncoder()
    author_mapped_ids = author_encoder.fit_transform(relevant_authors_raw)
    num_authors = len(author_encoder.classes_)
    
    item2author_array = author_mapped_ids
    valid_author_raw_set = set(author_encoder.classes_) # ç”¨äºç¤¾äº¤è¿‡æ»¤
    
    print(f"   ç»Ÿè®¡: Users={num_users}, Items={num_items}, Authors={num_authors}")

    # 4. User Features
    print("--- [4/6] ç”¨æˆ·æ´»è·ƒåº¦ç‰¹å¾ ---")
    df_user_feat = pd.read_csv(os.path.join(DATA_DIR, 'user_features.csv'))
    df_user_feat['user_active_degree'] = df_user_feat['user_active_degree'].fillna('unknown')
    u_feat_map = df_user_feat.set_index('user_id')['user_active_degree'].to_dict()
    
    active_encoder = LabelEncoder()
    all_labels = list(df_user_feat['user_active_degree'].unique())
    if 'unknown' not in all_labels: all_labels.append('unknown')
    active_encoder.fit(all_labels)
    num_active_levels = len(active_encoder.classes_)
    unknown_code = active_encoder.transform(['unknown'])[0]
    
    user_active_feature = np.full(num_users, unknown_code, dtype=int)
    # è¿™é‡Œè¦å°å¿ƒï¼šuser_encoder.classes_ æ˜¯ int è¿˜æ˜¯ strï¼ŸBigMatrix é‡Œé€šå¸¸æ˜¯ int
    # ç¡®ä¿ç±»å‹åŒ¹é…
    sample_feat_key = list(u_feat_map.keys())[0]
    sample_enc_key = user_encoder.classes_[0]
    
    # ç±»å‹è½¬æ¢æ£€æµ‹
    need_str_convert = isinstance(sample_feat_key, str) and not isinstance(sample_enc_key, str)
    
    for i, u_raw in enumerate(user_encoder.classes_):
        key = str(u_raw) if need_str_convert else u_raw
        if key in u_feat_map:
            user_active_feature[i] = active_encoder.transform([u_feat_map[key]])[0]

    # 5. ç¤¾äº¤å…³ç³» (çœŸå®å…¨é‡)
    print("--- [5/6] æå–ç¤¾äº¤å…³ç³» (Real Data Only) ---")
    # è¿™é‡Œæˆ‘ä»¬åšå†³ä¸é€ å‡æ•°æ®ï¼Œåªæå–çœŸå®çš„
    df_social = pd.read_csv(os.path.join(DATA_DIR, 'social_network.csv'))
    
    social_edges = []
    valid_user_raw_set = set(user_encoder.classes_)
    
    # ä¼˜åŒ–å¾ªç¯é€Ÿåº¦
    df_social_filtered = df_social[df_social['user_id'].isin(valid_user_raw_set)]
    
    print(f"    æ­£åœ¨æ‰«æ {len(df_social_filtered)} ä¸ªç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨...")
    
    for _, row in df_social_filtered.iterrows():
        u_raw = row['user_id']
        try:
            friend_list = literal_eval(row['friend_list'])
        except:
            continue
            
        u_idx = user_encoder.transform([u_raw])[0]
        
        for f_raw in friend_list:
            if f_raw in valid_author_raw_set:
                a_idx = author_encoder.transform([f_raw])[0]
                social_edges.append([u_idx, a_idx])
                
    social_edges = np.array(social_edges)
    # å»é‡
    if len(social_edges) > 0:
        social_edges = np.unique(social_edges, axis=0)
        
    print(f"âœ… æˆåŠŸæå–çœŸå®ç¤¾äº¤è¾¹: {len(social_edges)} æ¡")

    # 6. åˆ†å±‚ä¸ä¿å­˜
    print("--- [6/6] åˆ†å±‚ä¸ä¿å­˜ ---")
    author_heat = np.zeros(num_authors, dtype=int)
    item_counts = df_inter['item_idx'].value_counts()
    for i_idx, cnt in item_counts.items():
        author_heat[item2author_array[i_idx]] += cnt
        
    sorted_idx = np.argsort(author_heat)[::-1]
    n_head = int(num_authors * HEAD_RATIO)
    n_mid = int(num_authors * MID_RATIO)
    
    author_groups = np.zeros(num_authors, dtype=int)
    author_groups[sorted_idx[n_head:n_head+n_mid]] = 1
    author_groups[sorted_idx[:n_head]] = 2
    
    # åˆ‡åˆ†
    indices = np.arange(len(df_inter))
    np.random.shuffle(indices)
    split = int(len(indices) * 0.8)
    
    dataset = {
        'num_users': int(num_users),
        'num_items': int(num_items),
        'num_authors': int(num_authors),
        'num_active_levels': int(num_active_levels),
        'item2author': item2author_array,
        'user_active_feature': user_active_feature,
        'author_groups': author_groups,
        'social_edges': social_edges,
        'train_pairs': df_inter.iloc[indices[:split]][['user_idx', 'item_idx']].values,
        'test_pairs': df_inter.iloc[indices[split:]][['user_idx', 'item_idx']].values
    }
    
    with open(os.path.join(OUTPUT_DIR, OUTPUT_FILE), 'wb') as f:
        pickle.dump(dataset, f)
        
    print(f"ğŸ‰ å…¨é‡æ•°æ®å¤„ç†å®Œæˆï¼ä¿å­˜ä¸º: {OUTPUT_FILE}")

if __name__ == '__main__':
    process_data_big()