import pandas as pd
import numpy as np
import os
import pickle
from sklearn.preprocessing import LabelEncoder
from ast import literal_eval
import gc

# ================= 0. é…ç½®åŒº =================
DATA_DIR = './data'
OUTPUT_DIR = './processed_data'
SOURCE_FILE = 'big_matrix.csv' # ä½¿ç”¨å…¨é‡æ•°æ®
OUTPUT_FILE = 'dataset_hybrid.pkl' # è¾“å‡ºæ–‡ä»¶å

# --- æ ¸å¿ƒæŒ–æ˜å‚æ•° ---
# 1. éšå¼å…³æ³¨çš„æ•°é‡ä¸Šé™ï¼šæ¯ä¸ªç”¨æˆ·æœ€å¤šæŒ–æ˜ 15 ä¸ª
IMPLICIT_TOP_K = 15  

# 2. æœ€ä½äº’åŠ¨é˜ˆå€¼ï¼šåªæœ‰è·Ÿä½œè€…äº’åŠ¨(è§‚çœ‹) >= 2 æ¬¡æ‰ç®—"éšå¼å…³æ³¨"
# è¿™èƒ½æœ‰æ•ˆè¿‡æ»¤æ‰å¶ç„¶ç‚¹å‡»çš„å™ªç‚¹ï¼Œä¿è¯æå–å‡ºçš„ Top-20 éƒ½æ˜¯ç”¨æˆ·çœŸæ­£æ„Ÿå…´è¶£çš„
MIN_INTERACT_COUNT = 2 

# --- å…¶ä»–å‚æ•° ---
WATCH_RATIO_THRESHOLD = 0.5 
HEAD_RATIO = 0.2
MID_RATIO = 0.3
RANDOM_SEED = 2023
np.random.seed(RANDOM_SEED)

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_data_hybrid():
    print(f"ğŸš€ å¼€å§‹å…¨é‡æ•°æ®å¤„ç† (Hybrid: æ˜¾å¼ + éšå¼æŒ–æ˜)...")
    print(f"   ç­–ç•¥: Top-{IMPLICIT_TOP_K} ä¸” äº’åŠ¨æ¬¡æ•°>={MIN_INTERACT_COUNT}")

    # ================= 1. è¯»å–äº¤äº’ (Big Matrix) =================
    print(f"--- [1/6] è¯»å–äº¤äº’çŸ©é˜µ ({SOURCE_FILE}) ---")
    # ä¼˜åŒ–å†…å­˜ï¼šåªè¯»éœ€è¦çš„åˆ—
    use_cols = ['user_id', 'video_id', 'watch_ratio']
    df_inter = pd.read_csv(os.path.join(DATA_DIR, SOURCE_FILE), usecols=use_cols)
    
    # è¿‡æ»¤æ— æ•ˆäº¤äº’
    original_len = len(df_inter)
    df_inter = df_inter[df_inter['watch_ratio'] >= WATCH_RATIO_THRESHOLD].copy()
    print(f"    åŸå§‹æ•°æ®: {original_len} -> è¿‡æ»¤åæœ‰æ•ˆäº¤äº’: {len(df_inter)}")

    # ================= 2. å…³è”ä½œè€…ä¿¡æ¯ =================
    print("--- [2/6] å…³è”ä½œè€…ä¿¡æ¯ ---")
    feat_path = os.path.join(DATA_DIR, 'item_daily_features.csv')
    # åªè¯»ä¸¤åˆ—ï¼ŒèŠ‚çœå†…å­˜
    df_item_feat = pd.read_csv(feat_path, usecols=['video_id', 'author_id'])
    
    # å»é‡å¾—åˆ° video -> author æ˜ å°„
    item2author_raw = df_item_feat.drop_duplicates('video_id')[['video_id', 'author_id']]
    
    # è¿‡æ»¤ï¼šäº¤äº’è¡¨é‡Œçš„è§†é¢‘å¿…é¡»æœ‰ä½œè€…ä¿¡æ¯
    valid_videos = set(item2author_raw.video_id)
    df_inter = df_inter[df_inter['video_id'].isin(valid_videos)].copy()
    print(f"    å…³è”ä½œè€…åå‰©ä½™äº¤äº’: {len(df_inter)}")

    # é‡Šæ”¾å†…å­˜
    del df_item_feat
    gc.collect()

    # ================= 3. ID ç¼–ç  (User/Item/Author) =================
    print("--- [3/6] ID ç¼–ç  ---")
    
    # User
    user_encoder = LabelEncoder()
    df_inter['user_idx'] = user_encoder.fit_transform(df_inter['user_id'])
    
    # Item
    item_encoder = LabelEncoder()
    df_inter['item_idx'] = item_encoder.fit_transform(df_inter['video_id'])
    
    # Author
    # é€»è¾‘: video_id(str) -> author_id(str) -> author_idx(int)
    raw_vid2aid = dict(zip(item2author_raw.video_id, item2author_raw.author_id))
    
    # åªä¸ºåœ¨äº¤äº’ä¸­å‡ºç°çš„ Item å¯¹åº”çš„ Author è¿›è¡Œç¼–ç 
    relevant_authors_raw = [raw_vid2aid[vid] for vid in item_encoder.classes_]
    
    author_encoder = LabelEncoder()
    author_mapped_ids = author_encoder.fit_transform(relevant_authors_raw)
    
    # æ ¸å¿ƒæ˜ å°„æ•°ç»„: item_idx -> author_idx
    item2author_array = author_mapped_ids 
    
    num_users = len(user_encoder.classes_)
    num_items = len(item_encoder.classes_)
    num_authors = len(author_encoder.classes_)
    print(f"    Users: {num_users}, Items: {num_items}, Authors: {num_authors}")

    # ================= 4. æ··åˆç¤¾äº¤å…³ç³»æ„å»º (Hybrid) =================
    print(f"--- [4/6] æ„å»ºæ··åˆç¤¾äº¤ç½‘ç»œ ---")

    # ----- A. æŒ–æ˜éšå¼äº¤äº’ (Implicit Mining) -----
    print(f"    A. æŒ–æ˜éšå¼äº¤äº’ (Top-{IMPLICIT_TOP_K}, MinCount>={MIN_INTERACT_COUNT})...")
    
    # ç»™äº¤äº’è¡¨æ‰“ä¸Š author_idx
    df_inter['author_idx'] = item2author_array[df_inter['item_idx'].values]
    
    # èšåˆï¼šç»Ÿè®¡ (User, Author) çš„äº’åŠ¨æ¬¡æ•°
    # è¿™ä¸€æ­¥åœ¨å…¨é‡æ•°æ®ä¸Šå¯èƒ½ç¨æ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…
    print("       æ­£åœ¨èšåˆ User-Author äº¤äº’é¢‘æ¬¡...")
    user_author_counts = df_inter.groupby(['user_idx', 'author_idx']).size().reset_index(name='count')
    
    # [å…³é”®æ­¥éª¤] è¿‡æ»¤æ‰å¶ç„¶äº¤äº’ (åªä¿ç•™äº’åŠ¨ >= 2æ¬¡çš„)
    valid_interactions = user_author_counts[user_author_counts['count'] >= MIN_INTERACT_COUNT].copy()
    print(f"       è¿‡æ»¤ä½é¢‘äº¤äº’åï¼Œå‰©ä½™å€™é€‰å¯¹: {len(valid_interactions)}")
    
    # æ’åºï¼šæŒ‰äº’åŠ¨æ¬¡æ•°é™åº
    valid_interactions = valid_interactions.sort_values(['user_idx', 'count'], ascending=[True, False])
    
    # æˆªæ–­ï¼šæ¯ä¸ªç”¨æˆ·å– Top-K
    # æ³¨æ„ï¼šå¦‚æœç”¨æˆ·æœ‰æ•ˆäº¤äº’ä¸è¶³ K ä¸ªï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å–å®é™…ä¸ªæ•°ï¼Œä¸ä¼šæŠ¥é”™
    top_k_social = valid_interactions.groupby('user_idx').head(IMPLICIT_TOP_K)
    
    implicit_edges = top_k_social[['user_idx', 'author_idx']].values
    print(f"       >>> æŒ–æ˜å‡ºéšå¼è¾¹æ•°: {len(implicit_edges)}")

    # ----- B. æå–æ˜¾å¼å…³æ³¨ (Explicit Extraction) -----
    print(f"    B. æå–æ˜¾å¼å…³æ³¨ (social_network.csv)...")
    df_social = pd.read_csv(os.path.join(DATA_DIR, 'social_network.csv'))
    explicit_edges = []
    
    valid_user_set = set(user_encoder.classes_)
    valid_author_set = set(author_encoder.classes_)
    
    # åªå¤„ç†æœ‰æ•ˆç”¨æˆ·
    df_social = df_social[df_social['user_id'].isin(valid_user_set)]
    
    print(f"       æ­£åœ¨æ‰«æ {len(df_social)} ä¸ªç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨...")
    for _, row in df_social.iterrows():
        try:
            u_raw = row['user_id']
            friend_list = literal_eval(row['friend_list']) # è§£æåˆ—è¡¨å­—ç¬¦ä¸²
            
            u_idx = user_encoder.transform([u_raw])[0]
            
            for f_raw in friend_list:
                # åªæœ‰å…³æ³¨äº†å‘è¿‡è§†é¢‘çš„ä½œè€…ï¼Œæ‰ç®—æœ‰æ•ˆè¾¹
                if f_raw in valid_author_set:
                    a_idx = author_encoder.transform([f_raw])[0]
                    explicit_edges.append([u_idx, a_idx])
        except:
            continue
            
    explicit_edges = np.array(explicit_edges)
    print(f"       >>> æå–å‡ºæ˜¾å¼è¾¹æ•°: {len(explicit_edges)}")

    # ----- C. åˆå¹¶ä¸å»é‡ (Merge) -----
    print(f"    C. åˆå¹¶å»é‡...")
    if len(explicit_edges) > 0 and len(implicit_edges) > 0:
        social_edges = np.vstack([implicit_edges, explicit_edges])
    elif len(implicit_edges) > 0:
        social_edges = implicit_edges
    elif len(explicit_edges) > 0:
        social_edges = explicit_edges
    else:
        social_edges = np.empty((0, 2), dtype=int)
        
    # å»é‡ï¼šå¦‚æœåŒä¸€å¯¹å…³ç³»åœ¨æ˜¾å¼å’Œéšå¼é‡Œéƒ½å‡ºç°äº†ï¼Œåªç•™ä¸€æ¡
    social_edges = np.unique(social_edges, axis=0)
    
    print(f"âœ… æ··åˆç¤¾äº¤ç½‘ç»œæ„å»ºå®Œæˆï¼")
    print(f"   æœ€ç»ˆæ€»è¾¹æ•°: {len(social_edges)}")
    print(f"   å¹³å‡æ¯äººå…³æ³¨: {len(social_edges)/num_users:.2f} ä¸ªä½œè€…")

    # ================= 5. ç‰¹å¾å¤„ç† =================
    print("--- [5/6] ç‰¹å¾å¤„ç† ---")
    
    # ç”¨æˆ·æ´»è·ƒåº¦
    df_user_feat = pd.read_csv(os.path.join(DATA_DIR, 'user_features.csv'))
    df_user_feat['user_active_degree'] = df_user_feat['user_active_degree'].fillna('unknown')
    u_feat_map = df_user_feat.set_index('user_id')['user_active_degree'].to_dict()
    
    active_encoder = LabelEncoder()
    all_labels = list(df_user_feat['user_active_degree'].unique())
    if 'unknown' not in all_labels: all_labels.append('unknown')
    active_encoder.fit(all_labels)
    num_active_levels = len(active_encoder.classes_)
    unknown_code = active_encoder.transform(['unknown'])[0]
    
    user_active_feature = np.full(num_users, unknown_code, dtype=int)
    
    # ç±»å‹åŒ¹é…æ£€æµ‹ (é˜²æ­¢ str vs int å¯¼è‡´åŒ¹é…å¤±è´¥)
    sample_key = list(u_feat_map.keys())[0]
    need_str_convert = isinstance(sample_key, str) and not isinstance(user_encoder.classes_[0], str)
    
    for i, u_raw in enumerate(user_encoder.classes_):
        k = str(u_raw) if need_str_convert else u_raw
        if k in u_feat_map:
            user_active_feature[i] = active_encoder.transform([u_feat_map[k]])[0]

    # ä½œè€…çƒ­åº¦åˆ†å±‚
    author_heat = np.zeros(num_authors, dtype=int)
    # ä½¿ç”¨æ‰€æœ‰äº¤äº’ç»Ÿè®¡çƒ­åº¦
    total_author_counts = df_inter.groupby('author_idx').size()
    for aid, cnt in total_author_counts.items():
        author_heat[aid] = cnt
        
    sorted_idx = np.argsort(author_heat)[::-1]
    n_head = int(num_authors * HEAD_RATIO)
    n_mid = int(num_authors * MID_RATIO)
    
    author_groups = np.zeros(num_authors, dtype=int)
    author_groups[sorted_idx[n_head:n_head+n_mid]] = 1 # Mid
    author_groups[sorted_idx[:n_head]] = 2 # Head

    # ================= 6. åˆ‡åˆ†ä¸ä¿å­˜ =================
    print("--- [6/6] åˆ‡åˆ†æ•°æ®é›†ä¸ä¿å­˜ ---")
    indices = np.arange(len(df_inter))
    np.random.shuffle(indices)
    split = int(len(indices) * 0.8)
    
    dataset = {
        'num_users': int(num_users),
        'num_items': int(num_items),
        'num_authors': int(num_authors),
        'num_active_levels': int(num_active_levels),
        'item2author': item2author_array,
        'user_active_feature': user_active_feature,
        'author_groups': author_groups,
        'social_edges': social_edges, # æ··åˆåçš„è¾¹
        'train_pairs': df_inter.iloc[indices[:split]][['user_idx', 'item_idx']].values,
        'test_pairs': df_inter.iloc[indices[split:]][['user_idx', 'item_idx']].values
    }
    
    with open(os.path.join(OUTPUT_DIR, OUTPUT_FILE), 'wb') as f:
        pickle.dump(dataset, f)
        
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {os.path.join(OUTPUT_DIR, OUTPUT_FILE)}")

if __name__ == '__main__':
    process_data_hybrid()