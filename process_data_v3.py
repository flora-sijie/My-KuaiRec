import pandas as pd
import numpy as np
import pickle
import os
from sklearn.preprocessing import LabelEncoder

# ================= é…ç½®åŒº =================
# ç¡®ä¿è¿™é‡Œæ˜¯ä½ çš„æ–‡ä»¶è·¯å¾„
RAW_DATA_PATH = '/content/drive/MyDrive/MyCode/data/KuaiRec/data' 
OUTPUT_PATH = './processed_data/dataset.pkl'

def process_data_v3():
    print("ğŸš€ å¼€å§‹æ•°æ®å¤„ç† v3 (ä¿®å¤ç¤¾äº¤å…³ç³» ID å¯¹é½é—®é¢˜)...")
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    # Big Matrix (å…¨é‡) æˆ–è€… Small Matrix (å¼€å‘)
    # å»ºè®®ç›´æ¥ç”¨ big_matrixï¼Œå¦‚æœå†…å­˜å¤Ÿçš„è¯ï¼›ä¸ºäº†å¤ç°é—®é¢˜å…ˆç”¨ small
    df_matrix = pd.read_csv(os.path.join(RAW_DATA_PATH, 'small_matrix.csv')) 
    df_social = pd.read_csv(os.path.join(RAW_DATA_PATH, 'social_network.csv'))
    
    # 2. æå–æ ¸å¿ƒåˆ—
    # å‡è®¾ matrix åˆ—å: user_id, video_id, author_id, ...
    # å‡è®¾ social åˆ—å: user_id, friend_id (æ³¨æ„ï¼šfriend_id å°±æ˜¯è¢«å…³æ³¨è€…)
    
    # 3. æ„å»º ID Encoders
    print("--- æ„å»º ID æ˜ å°„ ---")
    
    # User Encoder: åŸºäºäº¤äº’çŸ©é˜µé‡Œçš„æ‰€æœ‰ç”¨æˆ·
    user_le = LabelEncoder()
    all_users = df_matrix['user_id'].unique()
    user_le.fit(all_users)
    
    # Item Encoder
    item_le = LabelEncoder()
    item_le.fit(df_matrix['video_id'].unique())
    
    # Author Encoder: åŸºäºäº¤äº’çŸ©é˜µé‡Œçš„æ‰€æœ‰ä½œè€…
    author_le = LabelEncoder()
    all_authors = df_matrix['author_id'].unique()
    author_le.fit(all_authors)
    
    print(f"Users: {len(user_le.classes_)}")
    print(f"Items: {len(item_le.classes_)}")
    print(f"Authors: {len(author_le.classes_)}")
    
    # 4. è½¬æ¢äº¤äº’çŸ©é˜µ ID
    print("--- è½¬æ¢äº¤äº’çŸ©é˜µ ---")
    df_matrix['u_idx'] = user_le.transform(df_matrix['user_id'])
    df_matrix['i_idx'] = item_le.transform(df_matrix['video_id'])
    df_matrix['a_idx'] = author_le.transform(df_matrix['author_id'])
    
    # 5. æ„å»º Item -> Author æ˜ å°„è¡¨ (æ¨¡å‹éœ€è¦)
    # é€»è¾‘: æ¯ä¸ª Item åªæœ‰ä¸€ä¸ª Author
    item2author_df = df_matrix[['i_idx', 'a_idx']].drop_duplicates().sort_values('i_idx')
    # ç¡®ä¿ item ID æ˜¯è¿ç»­çš„ï¼Œå¯ä»¥ç›´æ¥ç”¨ array ç´¢å¼•
    item2author_map = np.zeros(len(item_le.classes_), dtype=np.int64)
    item2author_map[item2author_df['i_idx'].values] = item2author_df['a_idx'].values
    
    # 6. å¤„ç†ç¤¾äº¤å…³ç³» (å…³é”®ä¿®å¤ç‚¹!!!)
    print("--- å¤„ç†ç¤¾äº¤å…³ç³» (Alignment) ---")
    
    # è¿‡æ»¤ 1: åªä¿ç•™åœ¨æˆ‘ä»¬ dataset ç”¨æˆ·åˆ—è¡¨é‡Œçš„ follower
    valid_followers = df_social['user_id'].isin(user_le.classes_)
    # è¿‡æ»¤ 2: åªä¿ç•™åœ¨æˆ‘ä»¬ dataset ä½œè€…åˆ—è¡¨é‡Œçš„ followee (è¢«å…³æ³¨è€…)
    # å…³é”®ï¼šæˆ‘ä»¬åªå…³å¿ƒâ€œå…³æ³¨äº†åœ¨è¿™ä¸ªæ•°æ®é›†é‡Œå‘è§†é¢‘çš„äººâ€
    valid_followees = df_social['friend_id'].isin(author_le.classes_)
    
    df_social_valid = df_social[valid_followers & valid_followees].copy()
    
    if len(df_social_valid) == 0:
        print("âš ï¸ è­¦å‘Š: Small Matrix å¤ªå°ï¼Œè¿‡æ»¤åæ²¡æœ‰å‰©ä½™ç¤¾äº¤å…³ç³»ã€‚å»ºè®®æ¢ Big Matrixã€‚")
        social_edges = []
    else:
        # æ ¸å¿ƒæ˜ å°„ï¼š
        # Follower -> User ID Space
        # Followee -> Author ID Space
        u_social = user_le.transform(df_social_valid['user_id'])
        a_social = author_le.transform(df_social_valid['friend_id'])
        
        social_edges = list(zip(u_social, a_social))
        print(f"âœ… æˆåŠŸæå–ç¤¾äº¤è¾¹: {len(social_edges)} æ¡")
        print(f"   (æ ¼å¼: User {u_social[0]} -> Author {a_social[0]})")

    # 7. å¤„ç†åˆ›ä½œè€…åˆ†å±‚ (Head/Tail) - ä¸ºäº† Manager
    print("--- è®¡ç®—åˆ›ä½œè€…åˆ†å±‚ ---")
    author_counts = df_matrix['a_idx'].value_counts()
    # è¿™é‡Œçš„é˜ˆå€¼å¯ä»¥æŒ‰åˆ†ä½æ•°å®šï¼Œæ¯”å¦‚å 50% æ˜¯ Tail
    tail_threshold = author_counts.quantile(0.5) 
    
    # 0: Tail, 1: Mid, 2: Head
    # å…ˆé»˜è®¤å…¨ä¸º 0
    author_groups = np.zeros(len(author_le.classes_), dtype=np.int64)
    
    for aid, count in author_counts.items():
        if count <= tail_threshold:
            author_groups[aid] = 0 # Tail
        elif count <= author_counts.quantile(0.8):
            author_groups[aid] = 1 # Mid
        else:
            author_groups[aid] = 2 # Head
            
    print(f"Tail Authors: {(author_groups==0).sum()}")

    # 8. å¤„ç†ç”¨æˆ·æ´»è·ƒåº¦ (User Active Level)
    # ç®€å•èµ·è§ï¼ŒæŒ‰äº¤äº’æ•°é‡åˆ† 4 æ¡£
    user_counts = df_matrix['u_idx'].value_counts()
    user_active_feature = np.zeros(len(user_le.classes_), dtype=np.int64)
    # åˆ†ä½æ•°: 0-40%, 40-70%, 70-90%, 90-100%
    q40 = user_counts.quantile(0.4)
    q70 = user_counts.quantile(0.7)
    q90 = user_counts.quantile(0.9)
    
    for uid, count in user_counts.items():
        if count <= q40: user_active_feature[uid] = 0
        elif count <= q70: user_active_feature[uid] = 1
        elif count <= q90: user_active_feature[uid] = 2
        else: user_active_feature[uid] = 3

    # 9. åˆ‡åˆ† Train/Test
    # ç®€å• Leave-one-out æˆ– 8:2
    print("--- åˆ‡åˆ†æ•°æ®é›† ---")
    # è¿™é‡Œç®€å•åšéšæœºåˆ‡åˆ†ï¼Œå®é™…å¯ç”¨æ—¶é—´æˆ³
    all_indices = np.random.permutation(len(df_matrix))
    train_size = int(len(df_matrix) * 0.8)
    train_idx = all_indices[:train_size]
    test_idx = all_indices[train_size:]
    
    train_pairs = list(zip(df_matrix['u_idx'].values[train_idx], df_matrix['i_idx'].values[train_idx]))
    test_pairs = list(zip(df_matrix['u_idx'].values[test_idx], df_matrix['i_idx'].values[test_idx]))

    # 10. ä¿å­˜
    data = {
        'num_users': len(user_le.classes_),
        'num_items': len(item_le.classes_),
        'num_authors': len(author_le.classes_),
        'num_active_levels': 4,
        'item2author': item2author_map,
        'author_groups': author_groups,
        'user_active_feature': user_active_feature,
        'train_pairs': train_pairs,
        'test_pairs': test_pairs,
        'social_edges': social_edges  # è¿™é‡Œçš„è¾¹å·²ç»æ˜¯ User->Author æ ¼å¼äº†
    }
    
    if not os.path.exists(os.path.dirname(OUTPUT_PATH)):
        os.makedirs(os.path.dirname(OUTPUT_PATH))
        
    with open(OUTPUT_PATH, 'wb') as f:
        pickle.dump(data, f)
        
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼ä¿å­˜è‡³: {OUTPUT_PATH}")

if __name__ == '__main__':
    process_data_v3()